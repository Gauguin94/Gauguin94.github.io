---
layout: post
---
<img src="/images/fulls/pls_latent_3d.JPG" style="width:413px; height:282px;">


# **대회 개요**
---
> [Credit Card Fraud Detection - Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
>
>
> 카드사는 사기성 신용 카드 거래를 인식하여  
> 고객이 구매하지 않은 항목에 대해 요금이 청구되지 않도록
> 하는 것이 중요하다.  
>  
> ## **데이터 개요**
---
<img src="/images/fulls/data_shape.png" style="width:933px; height:100px;">  

>> 284,807 건의 거래 중 사기(이상치) 거래는 단 "492"건만이 존재한다.  
>> 이를 퍼센티지로 따지면 0.172%이다.  
>> 데이터들의 Feature는 Time, V1 ~ V28, Amount, Class이다.  
>> 거래 데이터의 특성 상 보안 문제를 야기할 수 있기 때문에,  
>> 거래 데이터를 온전하게 표현할 수 없다.  
>> 그리하여 거래 관련 데이터에 PCA(주성분 분석)를 적용하여   
>> V1 ~ V28의 Feature로 나타내었다.  
>> Time, Amount, Class는 당연히 PCA가 적용되지 않았다.  
>> Class는 0, 1로 이루어져 있으며, 0은 정상, 1은 사기 거래를 나타낸다.   
>> 클래스 간 불균형이 매우 심하기 때문에,  
>> 개최자는(데이터 제공자는) Precision-Recall Curve를  
>> 사용하여 평가하는 것을 권장하였다.  
>> 위 사진은 상위 샘플 5개를 나타낸 것이다.
>
> ## **데이터 간단히 살펴보기**
---
<img src="/images/fulls/data_corr.png" style="width:85px; height:369px;">  

>> 필자가 생각하기에는, 거래와 같은 이상 탐지에서는  
>> "시간"이라는 특성이 매우 큰 영향을 끼친다고 생각한다.  
>> 예를 들어, 어제까지 10년간 하루에 100원만 쓰던 사람이 갑자기  
>> 오늘 10,000원을 쓴다면 이상 거래를 의심해볼만 하다.  
>> **왜?**  
>> 지금껏 하루에 100원만 썼는데("지금껏"이라는 시간의 개념.)  
>> 하루 아침에 달라졌기 때문이다.  
>> 하지만 이번에 다룰 데이터에서는 거래에 관련된 정보들에 이미 PCA를 적용하여  
>> V<sub>n</sub>에 해당하는 Feature로 함축시켰다.  
>> 그렇다면, 이미 시간 개념이 포함되어 있는 거래 관련 정보들이 모두  
>> PCA로 차원 축소되어 있다고 생각하여 이번 이상치 탐지에 크게 연관이 없을 것이라고  
>> 생각하였고, 때마침 상관성도 옅게 도출되었기 때문에  
>> Time은 과감하게 제외하고 분석을 진행하였다.  
>> **(위의 사진이 Class와 다른 Feature 간 상관도를 나타낸다.)**
  
# **도전**
---
<img src="/images/fulls/ae_pic.png" style="width:959px; height:449px;">  

> 이상치를 탐지하는 전형적인 방법들은  
> z-score를 통한 판단, K-nearest neighbor, k-means, isolation forest 등  
> 지도학습과 비지도학습을 아울러 여러 가지가 존재한다.  
> 그 중에서 가장 매력적인 방법이라고 생각되는 Auto Encoder(오토 인코더)를  
> 사용하여 데이터 분석을 진행하였다.  
> 오토인코더의 큰 틀에서의 개념은 매우 쉽다.  
> 그저 신경망의 입력으로 'A'라는 데이터를 집어 넣고  
> 신경망의 출력으로 'A'라는 데이터가 똑같이 나오도록 학습시키는 것이다.  
> **똑같이 나오도록 학습시키는게 이상 탐지랑 뭔 상관?**  
> 앞서 z-score, knn, k-means, isolation forest등 이상 탐지를 진행하는  
> 전형적인 방법들을 나열하였다.  
> 이들의 공통점이 무엇인가 하면,  
> 정상 데이터들과 이상 데이터들 간  
> 어떠한 차이가 있다는 것을 전제로 한다는 것이다.  
> 그리하여 정상 데이터들을 서로 군집시키거나, 이상 데이터를 고립시키는  
> 방법 등을 사용하여 정상 데이터와 이상 데이터를 분류할 수 있도록 한다.  
> **그렇다면,**  
> 오토인코더는 말 그대로 데이터 복원에 초점을 맞춰 학습을 진행하므로  
> 오토인코더로 하여금 정상 데이터만을 학습시켜 복원하도록 한다면,  
> 새로운 정상 데이터를 집어 넣게 되었을 경우에는 성공적으로 복원을 할 것이며,  
> 이상 데이터를 집어 넣게 되었을 경우에는 복원을 제대로  
> 진행하지 못할 것이라는 기대를 할 수 있다.  